---
title: "German Credit"
author: "Reza Dwi Utomo"
date: "03/03/2020"
output:
  html_document:
    highlight: zenburn
    number_sections: yes
    theme: flatly
    toc: yes
    toc_depth: 2
    toc_float:
      collapsed: yes
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction {#intro}

This article aims to accomplish [Classification in Machine Learning 1](https://algorit.ma/course/classification-1/) course at Algoritma. The dataset used is obtained from [the University of California at Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+(Sonar,+Mines+vs.+Rocks)), "Connectionist Bench (Sonar, Mines vs. Rocks)". You could see the source code fully in my GitHub account [here](https://github.com/utomoreza/C1_LBB).

## Aim

The goal is to model a prediction to decide between mines and rocks based on sonar signals data.

## Objectives

1. To compare between Logistic Regression and K-nearest neigbors performance

2. To assess the models by a binary classification metric, i.e. confusion matrix.

3. To interpretate the model (only for Logistic Regression)

4. To test the model using dataset test and discuss the results

## Structure

This article is arranged as follows.

1. [Introduction](#intro)
2. [Metadata](#meta)
3. [Preparation](#prep)
4. [Exploratory Data Analysis](#eda)
5. [Modelling and Predictions](#model)
6. [Evaluation](#eval)
6. [Model Tuning](#tuning)
7. [Discussions](#discus)
8. [Conclusions](#concl)

# Metadata {#meta}

## Content {#content}



## Relevant Paper


# Preparation {#prep}

Load all necessary packages.

```{r message=FALSE}
library(tidyverse) # for data wrangling
library(plotly) # for plotting using interactive plotly style
library(ggcorrplot) # for plotting correlation
library(GGally) # for plotting correlation
library(caret) # for confusion matrix
library(gtools) # for converting log of odds to probs
library(PerformanceAnalytics) # for pair plotting
library(car) # for executing VIF test
library(rsample) # for splitting dataset into train and test with controlled proportion
library(class) # for KNN
library(ROCR) # for calculating ROC
library(MLmetrics) # for calculating accuracy
```

Import the dataset.

```{r}
german <- read.csv("german_credit_data.csv")
head(german)
  ```

Since the dataset originally have no any column name, we should set it. As described in [Introduction](#content), the dataset consists of 60 numbers of energy and 1 target variable. Therefore, for such 60 numbers, we could name them as `energy1`, `energy`, so on and so forth, and for the target, we could simply name it as `type`.

```{r}
colnames(sonar) <- c(paste0("energy",c(1:60)), "type")
head(sonar)
```

# Data Wrangling

Firstly, let's check if NA exists.

```{r}
anyNA(sonar)
```

Unsurprisingly, as stated in the UCI Repository that the dataset does not contain missing values, we clearly did not find any NA value. Next, let's see the dataset structure.

```{r}
str(sonar)
```

All variables have their matched data type. Good. Now, move on to explore the dataset.

# Exploratory Data Analysis

Here, we're going to check the proportion of target variable and whether multicollinearity exists.

## Target Variable Proportion

We'd like to have adequately balanced proportion in each class of the target variable. Our target variable is `type`. Let's see its proportion.

```{r}
table(sonar$type)
prop.table(table(sonar$type))
```

Nice. Both sides have sufficiently balanced proportion. So, we can move on to check the multicollinearity.

## Multicollinearity {#multi}

We'd like to have each variable independent to each other, meaning that each of which does not have high correlation with others. Therefore, high correlation found, theoretically this indicates multicollinearity. First thing first, we should see the information of each variable by calling `summary()` function.

```{r}
summary(sonar)
```

As explained in [Introduction](#content) that the values of predictor variables range between 0 and 1, from above summary, we can see most variables share nearly the same range. Nevertheless, to make sure, let's see the predictors in boxplots.

```{r warning=FALSE}
p <- sonar %>% select(colnames(.)[-61]) %>%
  pivot_longer(cols = colnames(.), names_to = "Energy", values_to = "Value") %>%
  arrange(Energy) %>% 
  mutate(Energy = as.factor(Energy)) %>%
  ggplot(aes(x = Energy,
             y = Value)) +
  coord_flip() +
  geom_boxplot(aes(fill = Energy), show.legend = F) +
  theme(axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        legend.position = "none")
ggplotly(p)
```

{#boxplot}Eventhough the [metadata](#content) said that all predictors range from 0 from 1, we still found there are several of them having relatively minor values compared to the majority. We also found that there is no any high value beyond value of 1. Now, let's check their correlations.

```{r}
temp <- sonar %>% select(-type)
colnames(temp) <- c(1:60)
ggcorrplot(temp %>% cor())
```

There is a unique pattern discovered from above figure. Each variable has large positive correlation with their next neighbors. Therefore, we have to remove this pattern since high correlation indicates multicollinearity. We can drop the adjacent neighbors of each variable by only taking the columns with multiplication of 3, 4, 5, or 6. For example, if multiplication of 3 selected, we could take the variables of `energy1`, `energy3`, `energy6`, and all the way to `energy60`. As a beginning, I pick the multiplication of 4 to divide the number of predictor variables.

```{r}
idx <- c(1, seq(from = 4, to = ncol(temp), by = 4))
ggcorr(temp[,idx], label = T)
DF <- sonar[,c(idx,61)] # subset only necessary columns, i.e. the multipl of 4
```

Nice. We have removed all with high correlation. Now, all remaining are only variables with lower correlation. We also should check the p-value of correlation test.

```{r}
sonarComb <- combn(colnames(DF[,-ncol(DF)]), 2) # create combinations among each variable
Alpha <- 0.05 # set significance value
multicolRes <- data.frame(vs = 1:dim(sonarComb)[2], # create a blank data frame
                          cor = 1:dim(sonarComb)[2],
                          res = 1:dim(sonarComb)[2])
for (i in 1:dim(sonarComb)[2]) {
  multicolRes$vs[i] <- paste0(sonarComb[1,i], " & ", sonarComb[2,i])
  corTest <- cor.test(DF[,sonarComb[1,i]], DF[,sonarComb[2,i]])
  multicolRes$cor[i] <- corTest$p.value
  multicolRes$res[i] <- ifelse(corTest$p.value < Alpha, 
                               "Yes", 
                               "No")
  # multiResults <- c(multiResults, res)
}
head(multicolRes)
```

Too bad. The first five combinations show multicollinearity exists. We should check the composition.

```{r}
table(multicolRes$res)
prop.table(table(multicolRes$res))
```

We only have 44 combinations without multicollinearity detected. However, we just keep forward with the variables used so far.

# Modelling

In this chapter, we're going to split the dataset into train dataset and test dataset. Next, we will create two models of logistic regression and KNN (actually we won't create KNN model as it basically doesn't generate one). After the models retrieved, we will predict the test dataset using both models, and evalute the results.

## Splitting

In order to secure the proportion of two datasets after splitting, we're going to use `rsample` library here. The initial dataset used to split is `DF`, i.e. the one with removed variables of multiplication of 4.

```{r}
set.seed(1)
idx <- initial_split(DF, prop = 0.8, strata = type)
sonar_train <- training(idx)
sonar_test <- testing(idx)

prop.table(table(sonar_train$type)) # Check train dataset proportion after split
prop.table(table(sonar_test$type)) # Check test dataset proportion after split

# Split the predictors and the target of train dataset for KNN model usage
X_train <- sonar_train[,-ncol(sonar_train)]
y_train <- sonar_train[,ncol(sonar_train)]

# Split the predictors and the target of test dataset for KNN model usage
X_test <- sonar_test[,-ncol(sonar_test)]
y_test <- sonar_test[,ncol(sonar_test)]
```

Nice. The proportion of two datasets approximately is kept. Let's move on to create the models.

## Create the Model

### Logistic Regression Model {#logresmodel}

We're going to use all predictor variables.

```{r}
model_log <- glm(formula = type ~ ., data = sonar_train, family = "binomial", maxit = 30)
summary(model_log)
```

We can see above summary that there are five predictors without any significant code, namely `energy8`, `energy28`, `energy32`, `energy40`, and `energy60`. We will handle this issue later in [Model Tuning](#tuning). For now, let's predict the test dataset using the just-generated model.

### KNN Data Pre-Processing - Scalling

Specially for KNN, we need to scale all predictor variables in advance. Though all of them are [said](#content) to be in range between 0 and 1, we [found earlier](#multi) that some spread not in range exactly 0 and 1. Thus, in order to ensure, it's preferable to still scale all of them.

```{r}
X_train.scaled <- scale(x = X_train)
X_test.scaled <- scale(x = X_test, 
                       center = attr(X_train.scaled, "scaled:center"),
                       scale = attr(X_train.scaled, "scaled:scale"))
```

## Predictions

After the models have been prepared. Let's predict the test dataset.

### Logistic Regression Prediction

Since the direct output of logistic regression model is in log of odds form, we need to convert it to a more interpretable form, i.e. probability. We can use the argument `type = "response"` to arrange this. Afterwards, all probabilities have to be manually assigned to whether positive (Rocks) or negative (Mines) based on the set threshold. As a start, we could use threshold of 0.5.

```{r}
predict_log <- predict(object = model_log, newdata = sonar_test, type = "response")
sonar_test$ypred_prob <- predict_log
sonar_test$ypred_label <- ifelse(sonar_test$ypred_prob > 0.5, "R", "M")
# negative class is M
# positive class is R
# since at the beginning, R automatically sets "M" as the first as class, meaning the negative class
```

### KNN Prediction

To predict the test dataset using KNN, firstly, we have to determine initial optimum K value. In this case, we're going to obtain K by calculating the square root of all observations of the train dataset.

* Find optimum `K`

```{r}
K <- sqrt(nrow(X_train))
K
```

As the target variable has two classes only (`R` or `M`) meaning that it is an even number, we need an odd number of `K`. Therefore, with `K = 13`, it should be enough for the prediction.

```{r}
predict_knn <- knn(train = X_train.scaled, test = X_test.scaled, cl = y_train, k = round(K))
```

## Evaluation {#eval}

Both models have been used to predict the test dataset. Now, let's evaluate them. Here, to assess both, we're going to utilize [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix). For confusion matrix metric, I'm only interested to highlight the Accuracy, Sensitivity (Recall), and Pos Pred Value (Precision). The figure below briefly shows the confusion matrix and its attributes.

<center>
![Confusion Matrix Table](https://2.bp.blogspot.com/-EvSXDotTOwc/XMfeOGZ-CVI/AAAAAAAAEiE/oePFfvhfOQM11dgRn9FkPxlegCXbgOF4QCLcBGAs/s1600/confusionMatrxiUpdated.jpg)
</center>

Let's compare both models.

```{r}
print("Confusion Matrix of Log Regression Model")
confusionMatrix(data = as.factor(sonar_test$ypred_label), reference = y_test, positive = "R")
print("Confusion Matrix of KNN")
confusionMatrix(data = predict_knn, reference = y_test, positive = "R")
```

Unfortunately, both models perform poorly. All points we're interested in (i.e. Accuracy, Sensitivity, and Pos Pred Value) are more or less below 7. Though, the logistic regression model slightly produces superior results. Based on this insufficient performance, the models are required to tune that both can perform more satisfactorily.

# Model Tuning {#tuning}

## Tuning Logistic Regression

As mentioned [previously](#logresmodel), there are several predictor variables (i.e. `energy8`, `energy28`, `energy32`, `energy40`, and `energy60`) which are insignificant in the logistic regression model. Now, we're going to remove them, and it is expected that this could enhance the model's performance.

```{r}
energyInsignificant <- c("energy8", "energy28", "energy32", "energy40", "energy60")
sonar_train2 <- sonar_train %>% select(-all_of(energyInsignificant))
```

New dataset has been created already. Now, let's use it to train the model and see the results.

```{r}
model_log2 <- glm(formula = type ~ ., data = sonar_train2, family = "binomial", maxit = 30)
summary(model_log2)
```

Nice. Each variable at least has `.` sign indicating that its p-value is less than 0.1. Now, use the new model to predict the test dataset.

```{r}
predict_log2 <- predict(object = model_log2, newdata = sonar_test, type = "response")
sonar_test$ypred_prob2 <- predict_log2
sonar_test$ypred_label2 <- ifelse(sonar_test$ypred_prob2 > 0.5, "R", "M")
```

Subsequently, let's see its performance by using confusion matrix.

```{r}
confusionMatrix(as.factor(sonar_test$ypred_label2), sonar_test$type, positive = "R")
```

Good. By eliminating all insignificant predictors, we could increase the performance by approxitamely 2%. To see the enhancement, let's compare all the initial model and the tuned model in the points we're interested in.

```{r}
LogResTable1 <- table(sonar_test$ypred_label, sonar_test$type, dnn = c("Prediction","Actual"))
LogResTable2 <- table(sonar_test$ypred_label2, sonar_test$type, dnn = c("Prediction","Actual"))

Accuracy1 <- (LogResTable1[1,1] + LogResTable1[2,2])/nrow(sonar_test)
Accuracy2 <- (LogResTable2[1,1] + LogResTable2[2,2])/nrow(sonar_test)
Recall1 <- LogResTable1[2,2]/(LogResTable1[1,2] + LogResTable1[2,2])
Recall2 <- LogResTable2[2,2]/(LogResTable2[1,2] + LogResTable2[2,2])
Precision1 <- LogResTable1[2,2]/(LogResTable1[2,1] + LogResTable1[2,2])
Precision2 <- LogResTable2[2,2]/(LogResTable2[2,1] + LogResTable2[2,2])

finalResults.LogRes <- data.frame("Model" = c("Initial Log Res","Tuned Log Res"),
                                  "Accuracy" = c(Accuracy1, Accuracy2),
                                  "Recall" = c(Recall1, Recall2),
                                  "Precision" = c(Precision1, Precision2))
cat("\nConfusion matrix of the initial model\n")
LogResTable1
cat("\nConfusion matrix of the tuned model\n")
LogResTable2
finalResults.LogRes
```

As seen above, the variable elimination is able to move exactly 1 False Negative (FN) from the results of the initial model to True Positive (TP) as exhibited by the tuned model. By this 1 FN-to-TP movement, the performance increases by more or less 2%, 5%, and 1%, as shown by the data frame above, for accuracy, recall, and precision, respectively. From now on, we use the tuned logistic regression model as our final model to compare to that of KNN.

## Tuning KNN

For tuning KNN, we could find the best K value. We could carry out this by using every possible odd K value to predict the test dataset and select the one with best accuracy. This technique is kind of brute force so it is not recommended for the dataset with long rows unless you have a computer with high specifications.

```{r}
# get all odd K values
oddK_idx <- sapply(1:(nrow(X_train)-1), function(x) {ifelse(x%%2 == 1, T, F)})
optK <- data.frame(1:(nrow(X_train)-1))[oddK_idx,]
optKNN <- data.frame(K = 1:length(optK), acc = 1:length(optK)) # create blank dataframe

# brute force to calculate every accuracy of all odd Ks
for (i in 1:length(optK)) {
  predknn <- knn(train = X_train.scaled, test = X_test.scaled, cl = y_train, k = optK[i])
  optKNN$K[i] <- optK[i]
  optKNN$acc[i] <- Accuracy(predknn, y_test)
}
maxAcc <- optKNN[optKNN$acc == max(optKNN$acc),] # find the one with max accuracy
labelMax <- paste0(round(maxAcc$acc, 3), " when K = ", maxAcc$K) # and label it

K13 <- optKNN[optKNN$K == 13,]
labelK13 <- "K = 13"

# plot the results
optKNN %>% ggplot(aes(x = K,
                      y = acc)) +
  geom_line(color = "blue") +
  geom_point(data = maxAcc, aes(x = K,
                                y = acc)) +
  geom_point(data = K13, aes(x = K,
                             y = acc)) +
  geom_text(data = maxAcc, 
             aes(label = labelMax), 
             hjust = -0.1, 
             size = 3.5) +
  geom_text(data = K13, aes(label = labelK13), size = 2.5, hjust = 1.4) +
  labs(title = "No of Ks vs Accuracy in KNN prediction",
       x = "Number of Ks",
       y = "Accuracy")
```

Suprisingly, the best K value obtained is just `1`. Even, this `K = 1` transcends all possible K values. As shown above, our previous K, i.e. `K = 13`, is far below `K = 1`. Based on this finding, we're going to reset K value of our KNN model and see its results.

```{r}
bestK <- maxAcc$K
predict_knn2 <- knn(train = X_train.scaled, test = X_test.scaled, cl = y_train, k = bestK)
confusionMatrix(predict_knn2, reference = y_test, positive = "R")
```

Great. Now, our KNN model reaches the accuracy of 80%. Let's compare this tuned model to the initial one.

```{r}
KNNTable1 <- table(predict_knn, y_test, dnn = c("Prediction","Actual"))
KNNTable2 <- table(predict_knn2, y_test, dnn = c("Prediction","Actual"))

Accuracy1 <- (KNNTable1[1,1] + KNNTable1[2,2])/nrow(sonar_test)
Accuracy2 <- (KNNTable2[1,1] + KNNTable2[2,2])/nrow(sonar_test)
Recall1 <- KNNTable1[2,2]/(KNNTable1[1,2] + KNNTable1[2,2])
Recall2 <- KNNTable2[2,2]/(KNNTable2[1,2] + KNNTable2[2,2])
Precision1 <- KNNTable1[2,2]/(KNNTable1[2,1] + KNNTable1[2,2])
Precision2 <- KNNTable2[2,2]/(KNNTable2[2,1] + KNNTable2[2,2])

finalResults.KNN <- data.frame("Model" = c("Initial KNN","Tuned KNN"),
                                  "Accuracy" = c(Accuracy1, Accuracy2),
                                  "Recall" = c(Recall1, Recall2),
                                  "Precision" = c(Precision1, Precision2))
cat("Confusion matrix of the initial model\n")
KNNTable1
cat("\nConfusion matrix of the tuned model\n")
KNNTable2
finalResults.KNN
```

Amazing. Such `K = 1` significantly elevates the performance of KNN model by approximately 20%, 26%, and 22%, as seen in above data frame, for accuracy, recall, and precision, respectively. This is proven by, if you see the confusion matrices above, three FP-to-TN and five FN-to-TP movements. From now on, the KNN model used is the tuned one with `K = 1`.

## Comparison of the Tuned Models

Both logistic regression and KNN models have been tuned. Both produce improved results. Now, let's compare both tuned models and see the results.

```{r}
finalResults <- rbind(finalResults.LogRes[2,], finalResults.KNN[2,])
rownames(finalResults) <- NULL
finalResults
```

As seen above, in case of accuracy and precision, the KNN performs much better than the logistic regression. However, uniquely, in case of recall, both models exhibit exactly the same values. This could occur because both have exactly the same proportion of true positive and false negative as shown by the table below.

```{r}
cat("Confusion Matrix of the tuned log res model\n")
LogResTable2
cat("\nConfusion matrix of the tuned KNN model\n")
KNNTable2
```

**Based on the results explained, we can conclude that, specially for this dataset, the KNN model performs more efficient than the logistic regression model**. Subsequently, since both models are unable to perform the accuracy of 90%, we could determine the most highlighted metric (whether recall or precision) based on the business case of the dataset.

* If we prefer high recall, this means that we increase the probability to have more false positives than false negatives. This could cause we might to have more observations predicted as rocks whereas originally they are mines.

* If we prefer high precision, this means that we increase the probability to have more false negatives than false positives. This could cause we might to have more observations predicted as mines whereas originally they are rocks.

# Conclusions {#concl}

Finally, you have read a lot. Thank you for reaching this much. Now, let's conclude this article. Firstly, we should conclude from what has been defined in [Introduction](#intro).

* We have satisfied the aim. Two methods, i.e. Logistic Regression and K-Nearest Neighbors, have been generated to decide whether the observation is mines or rocks based on sonar signals data.

* We have created comparisons between Logistic Regression and K-nearest neigbors performance.

* We have assessed both models based on confusion matrix.

* We have examined the model using test dataset and discussed the results.

* By applying model tuning technique, we have upgraded the performance of both models, namely:

  - by 2%, 5%, and 1% for accuracy, recall, and precision, respectively, for logistic regression
 
  - by 20%, 26%, and 22% for accuracy, recall, and precision, respectively, for KNN.
 
* After both final models compared each other, the final KNN model comes out as a champion. Though, it might be overfitted since the K value used is 1.

* Since both final models do not reach the accuracy of almost 100%, it is wiser if we highlight one of other metrics, either recall or precision, based on the business case of the dataset.